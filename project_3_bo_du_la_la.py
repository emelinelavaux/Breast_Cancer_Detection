# -*- coding: utf-8 -*-
"""Project_3_BDLL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18V59UorKu58OuiwI7oGCMsdJP08vsLwE

# Project 3

Bochard, Dumas, Labracherie, Lavaux
"""

# IMPORTS

# --- Python stdlib ---
import copy            # utilities for shallow/deep copies of Python objects
import math            # math functions (pi, exp, trig, etc.)
import random          # Python's built-in RNG for reproducibility and shuffling
import warnings        # control/ignore warnings in notebooks and scripts

# --- Core scientific stack & plotting ---
import numpy as np     # numerical computing with ndarrays
import pandas as pd    # data manipulation and tabular structures (DataFrame)
import matplotlib.pyplot as plt  # plotting and visualization

# --- Imaging / I/O / preprocessing ---
import cv2             # OpenCV: image processing (CLAHE, denoising, etc.)
from PIL import Image, ImageEnhance  # image loading and simple augmentation

# --- Feature engineering ---
from scipy.stats import entropy  # statistical measures (e.g., entropy)
from skimage.feature import graycomatrix, graycoprops, local_binary_pattern  # texture features (GLCM, LBP)
from skimage.filters import gabor  # Gabor filters for frequency/texture features

# --- Scikit-learn: preprocessing & feature selection ---
from sklearn.preprocessing import StandardScaler  # feature standardization
from sklearn.feature_selection import f_classif, mutual_info_classif, SelectKBest, RFE  # feature ranking/selection

# --- Scikit-learn: models ---
from sklearn.linear_model import LogisticRegression  # logistic regression classifier
from sklearn.svm import LinearSVC                    # linear SVM (for calibrated SVM)
from sklearn.calibration import CalibratedClassifierCV  # probability calibration (e.g., sigmoid/Platt scaling)
from sklearn.model_selection import StratifiedKFold # to keep class proportions

# --- Scikit-learn: metrics ---
from sklearn.metrics import (  # evaluation metrics and reports
    roc_auc_score,
    accuracy_score,
    f1_score,
    confusion_matrix,
    classification_report,
    precision_recall_curve
)

# --- PyTorch: core, NN modules & data pipeline ---
import torch                        # core PyTorch tensor library and device management
import torch.nn as nn               # neural network layers and modules
import torch.nn.functional as F     # functional API for activations / loss ops
from torch.utils.data import Dataset, DataLoader  # dataset/dataloader abstractions

# Import Database
d = np.load("breastmnist_128.npz")
print(d.files)
for k in d.files:
    a = d[k]
    print(f"{k:15s}", a.shape, a.dtype)

np.unique(d['train_labels'])

"""##### Dataset split into 3 sets: 546 images for training, 78 for validation, and 156 for testing. The labels are binary (0: normal/benign or 1: malignant)."""

# Data visualization:

X_train = d['train_images']          # (546, 128, 128), uint8
y_train = d['train_labels'].ravel()  # (546,), uint8

X_val   = d['val_images']
y_val   = d['val_labels'].ravel()

X_test  = d['test_images']
y_test  = d['test_labels'].ravel()

# Class mapping
label_names = {0: "normal/benign", 1: "malignant"}

# Random samples
np.random.seed(42)
idx = np.random.choice(len(X_train), size=12, replace=False)

cols = 6
rows = len(idx) // cols + (len(idx) % cols > 0)
plt.figure(figsize=(12, 6))
for i, k in enumerate(idx, 1):
    plt.subplot(rows, cols, i)
    plt.imshow(X_train[k], cmap='gray', vmin=0, vmax=255)
    plt.title(label_names[int(y_train[k])], fontsize=9)
    plt.axis('off')
plt.tight_layout()
plt.show()

vals, cnts = np.unique(y_train, return_counts=True)
for v, c in zip(vals, cnts):
    print(f"{label_names[int(v)]:>13s}: {c}")

"""# Part 1: Preprocessing


"""

# STEP 1: Tweak: CLAHE

def apply_clahe_uint8(x_uint8, clip=2.0, grid=(8,8)):
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=grid)
    return np.stack([clahe.apply(img) for img in x_uint8], axis=0).astype(np.uint8)

# Apply CLAHE on each data split (before scaling/normalization)
X_train_clahe = apply_clahe_uint8(X_train, clip=2.0, grid=(8,8))
X_val_clahe   = apply_clahe_uint8(X_val,   clip=2.0, grid=(8,8))
X_test_clahe  = apply_clahe_uint8(X_test,  clip=2.0, grid=(8,8))

k = np.random.randint(len(X_train))

img_orig  = X_train[k].astype(np.uint8)         # (128,128)
img_clahe = X_train_clahe[k].astype(np.uint8)   # (128,128)

plt.figure(figsize=(9, 6))

# Images
plt.subplot(2, 2, 1)
plt.imshow(img_orig, cmap='gray', vmin=0, vmax=255)
plt.title(f"Original (idx {k})")
plt.axis("off")

plt.subplot(2, 2, 2)
plt.imshow(img_clahe, cmap='gray', vmin=0, vmax=255)
plt.title("CLAHE")
plt.axis("off")

# Plot intensity histograms
plt.subplot(2, 2, 3)
plt.hist(img_orig.ravel(), bins=256, range=(0,255), alpha=0.8)
plt.title("Histogramme original")
plt.xlabel("Intensité"); plt.ylabel("Comptes")

plt.subplot(2, 2, 4)
plt.hist(img_clahe.ravel(), bins=256, range=(0,255), alpha=0.8)
plt.title("Histogramme après CLAHE")
plt.xlabel("Intensité"); plt.ylabel("Comptes")

plt.tight_layout()
plt.show()

# Another tweak: light denoising

# ---- 1) Function: apply a median filter on a uint8 batch (N, 128, 128)
def apply_median_blur(x_uint8, ksize=3):
    """
    x_uint8 : np.ndarray uint8, shape (N, H, W)
    ksize   : taille du noyau (IMPÉRATIF: impair, ex. 3, 5, 7)
    """
    return np.stack([cv2.medianBlur(img, ksize) for img in x_uint8], axis=0)

# ---- 2) Apply on the training split
X_train_med = apply_median_blur(X_train_clahe, ksize=3)
X_val_med   = apply_median_blur(X_val_clahe,   ksize=3)
X_test_med  = apply_median_blur(X_test_clahe,  ksize=3)

# ---- 3) Before/after visualization on a random image (from the training split)
k = np.random.randint(len(X_train_med))
orig  = X_train_clahe[k].astype(np.uint8) # image after CLAHE (reference)
med   = X_train_med[k].astype(np.uint8)

plt.figure(figsize=(9, 6))

# Images
plt.subplot(2, 2, 1)
plt.imshow(orig, cmap='gray', vmin=0, vmax=255)
plt.title(f"Original (idx {k})")
plt.axis("off")

plt.subplot(2, 2, 2)
plt.imshow(med, cmap='gray', vmin=0, vmax=255)
plt.title("Median blur (ksize=3)")
plt.axis("off")

# Histograms
plt.subplot(2, 2, 3)
plt.hist(orig.ravel(), bins=256, range=(0,255))
plt.title("Histogramme original")
plt.xlabel("Intensité"); plt.ylabel("Comptes")

plt.subplot(2, 2, 4)
plt.hist(med.ravel(), bins=256, range=(0,255))
plt.title("Histogramme après médian")
plt.xlabel("Intensité"); plt.ylabel("Comptes")

plt.tight_layout()
plt.show()

# ---- 4) Visual difference
plt.figure(figsize=(4,4))
diff = med.astype(np.int16) - orig.astype(np.int16)
plt.imshow(diff, cmap='bwr')
plt.title("Diff (median - original)")
plt.axis("off"); plt.colorbar(shrink=0.8)
plt.show()

# --- Pick a random training image (uint8, shape: HxW) ---
k = np.random.randint(len(X_train))
img_before = X_train[k].astype(np.uint8)

# --- Apply CLAHE + Median Blur ---
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
img_clahe = clahe.apply(img_before)
img_after  = cv2.medianBlur(img_clahe, ksize=3)

# --- Show side-by-side (for quick check) ---
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plt.imshow(img_before, cmap='gray', vmin=0, vmax=255)
plt.title("Before"); plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(img_after, cmap='gray', vmin=0, vmax=255)
plt.title("After (CLAHE + median)"); plt.axis("off")

plt.tight_layout()
plt.show()

# Loop applying data augmentation on the entire training set

def augment_image(img_np):
    """Applique plusieurs augmentations aléatoires sur une image uint8 (128x128)"""
    img = Image.fromarray(img_np)

    # Geometric
    if np.random.rand() < 0.5:
        img = img.transpose(Image.FLIP_LEFT_RIGHT)
    angle = np.random.uniform(-10, 10)
    img = img.rotate(angle, resample=Image.BILINEAR, fillcolor=0)
    factor = np.random.uniform(0.9, 1.1)
    w, h = img.size
    new_w, new_h = int(w * factor), int(h * factor)
    im_resized = img.resize((new_w, new_h), resample=Image.BILINEAR)
    if factor >= 1.0:
        left = (new_w - w) // 2
        top = (new_h - h) // 2
        img = im_resized.crop((left, top, left + w, top + h))
    else:
        canvas = Image.new(img.mode, (w, h), color=0)
        left = (w - new_w) // 2
        top = (h - new_h) // 2
        canvas.paste(im_resized, (left, top))
        img = canvas

    # Photometric
    if np.random.rand() < 0.5:
        img = ImageEnhance.Brightness(img).enhance(np.random.uniform(0.8, 1.2))
    if np.random.rand() < 0.5:
        img = ImageEnhance.Contrast(img).enhance(np.random.uniform(0.9, 1.3))

# --- SPECKLE noise ---
    if np.random.rand() < 0.3:
        x = np.array(img, dtype=np.float32) / 255.0
        noise = np.random.normal(0.0, 0.10, x.shape).astype(np.float32)  # sigma=0.10
        y = x * (1.0 + noise)
        y = np.clip(y, 0.0, 1.0) * 255.0
        img = Image.fromarray(y.astype(np.uint8))

    return np.array(img, dtype=np.uint8)


# --- Apply on the training set ---
X_train_aug = np.array([augment_image(img) for img in X_train_med], dtype=np.uint8)
y_train_aug = y_train.copy()

# Concatenate original + augmented data
X_train_all = np.concatenate([X_train_med, X_train_aug], axis=0)
y_train_all = np.concatenate([y_train, y_train_aug], axis=0)

print("Train set size before:", len(X_train))
print("Train set size after augmentation:", len(X_train_all))

# Visualization of original/augmented pairs
n = len(X_train_med)
idx = np.random.choice(n, size=5, replace=False)
plt.figure(figsize=(10, 6))
for i, k in enumerate(idx, 1):
    plt.subplot(5,2,2*i-1); plt.imshow(X_train_med[k], cmap='gray', vmin=0, vmax=255); plt.axis('off'); plt.title(f"Original (med) #{k}")
    plt.subplot(5,2,2*i);   plt.imshow(X_train_all[n+k], cmap='gray', vmin=0, vmax=255); plt.axis('off'); plt.title(f"Augmented #{k}")
plt.tight_layout(); plt.show()

# STEP 2: SCALING
# Convert to float32 and rescale between 0 and 1
X_train_scaled = X_train_all.astype('float32') / 255.0
X_val_scaled   = X_val_med.astype('float32') / 255.0
X_test_scaled  = X_test_med.astype('float32') / 255.0

# Verification
print("Train:", X_train_scaled.min(), X_train_scaled.max())
print("Val:", X_val_scaled.min(), X_val_scaled.max())
print("Test:", X_test_scaled.min(), X_test_scaled.max())

# === STEP 3: NORMALIZATION (z-score) ===
# Compute mean/std on the TRAIN set only (grayscale → single channel)
eps = 1e-8
train_mean = X_train_scaled.mean()                  # scalaire
train_std  = X_train_scaled.std() + eps             # scalaire

# Apply to all 3 splits
X_train_norm = (X_train_scaled - train_mean) / train_std
X_val_norm   = (X_val_scaled   - train_mean) / train_std
X_test_norm  = (X_test_scaled  - train_mean) / train_std

# Verification
print(f"train mean/std: {train_mean:.6f} / {train_std:.6f}")
print("Train min/max:", float(X_train_norm.min()), float(X_train_norm.max()))
print("Val   min/max:", float(X_val_norm.min()),   float(X_val_norm.max()))
print("Test  min/max:", float(X_test_norm.min()),  float(X_test_norm.max()))

#(Visualization) # === STEP 3: Data augmentation === (DO IT BEFORE STEPS ABOVE IN CASE YOU USE IT)

# === 1 — Geometric Transformations (rotation, flip)

img = Image.fromarray((X_train[0]).astype(np.uint8)) # to illustrate

# --- Geometric Augmentations ---
img_flp = img.transpose(Image.FLIP_LEFT_RIGHT)            # flip horizontal
img_rot = img.rotate(10, resample=Image.BILINEAR)         # rotation +10°

def zoom_image(img, factor=1.1):
    """
    Zoom in/out tout en revenant à la taille d'origine.
    factor > 1 : zoom avant (agrandit puis crop centre)
    factor < 1 : zoom arrière (réduit puis pad noir autour)
    """
    w, h = img.size
    new_w, new_h = int(w * factor), int(h * factor)
    im_resized = img.resize((new_w, new_h), resample=Image.BILINEAR)
    if factor >= 1.0:
        # crop au centre
        left = (new_w - w) // 2
        top  = (new_h - h) // 2
        return im_resized.crop((left, top, left + w, top + h))
    else:
        # pad au centre
        canvas = Image.new(img.mode, (w, h), color=0)
        left = (w - new_w) // 2
        top  = (h - new_h) // 2
        canvas.paste(im_resized, (left, top))
        return canvas
img_zoom  = zoom_image(img, factor=1.10)                         # zoom +10%

# Visualization
plt.figure(figsize=(10,4))
for i, (im, t) in enumerate([(img, "original"), (img_flp, "flip"), (img_rot, "rotate +15°"), (img_zoom, "zoom ×1.10")]):
    plt.subplot(1,4,i+1)
    plt.imshow(im, cmap='gray')
    plt.axis("off")
    plt.title(t)
plt.show()

# === 2 — Photometric Transformations (contraste, luminosité)

# --- Photometric Augmentations ---
img_brt = ImageEnhance.Brightness(img).enhance(1.3)    # +30% brightness
img_con = ImageEnhance.Contrast(img).enhance(1.5)      # +50% contrast

def add_speckle(img, sigma=0.10):
    """Bruit speckle (multiplicatif) sur image PIL en niveaux de gris."""
    x = np.array(img, dtype=np.float32) / 255.0
    noise = np.random.normal(0.0, sigma, x.shape).astype(np.float32)
    y = x * (1.0 + noise)
    y = np.clip(y, 0.0, 1.0) * 255.0
    return Image.fromarray(y.astype(np.uint8))

img_noise = add_speckle(img, sigma=0.10)

plt.figure(figsize=(10,3))
for i, (im, t) in enumerate([(img, "original"), (img_brt, "brightness ×1.3"), (img_con, "contrast ×1.5"), (img_noise, "gaussian noise σ=0.03")]):
    plt.subplot(1,4,i+1)
    plt.imshow(im, cmap='gray')
    plt.axis("off")
    plt.title(t)
plt.show()

"""# Part 2 - Features

"""

#uses the preprocessed data
try:
    X_train_u8 = X_train_med
    X_val_u8   = X_val_med
    X_test_u8  = X_test_med
except NameError:
    X_train_u8 = X_train.astype(np.uint8)
    X_val_u8   = X_val.astype(np.uint8)
    X_test_u8  = X_test.astype(np.uint8)

y_train_bin = np.asarray(y_train).ravel().astype(np.uint8)
y_val_bin   = np.asarray(y_val).ravel().astype(np.uint8)
y_test_bin  = np.asarray(y_test).ravel().astype(np.uint8)

#we reduce the number of gray levels so that the GLCM is not too large
def quantize_ubyte(img_u8, n_levels=32):
    """Quantifie 0..255 -> 0..(n_levels-1) pour stabiliser la GLCM."""
    binsize = 256 // n_levels
    q = (img_u8 // binsize).astype(np.uint8)
    q[q >= n_levels] = n_levels - 1
    return q

#extract tumor texture features from the GLCM
def glcm_features(img_u8, n_levels=32, distances=(1, 2), angles=(0, np.pi/4, np.pi/2, 3*np.pi/4)):
    """Features GLCM moyennées sur distances & angles."""
    q = quantize_ubyte(img_u8, n_levels=n_levels)
    glcm = graycomatrix(q, distances=distances, angles=angles, levels=n_levels, symmetric=True, normed=True)
    feats = {}
    props = ['contrast','homogeneity','energy','correlation','ASM','dissimilarity']
    for prop in props:
        try:
            vals = graycoprops(glcm, prop)
            feats[f'glcm_{prop}_mean'] = float(vals.mean())
            feats[f'glcm_{prop}_std']  = float(vals.std())
        except Exception:
            pass
    return feats

#capture local micro-texture using LBP (Local Binary Patterns)
def lbp_features(img_u8, P=8, R=1.0, n_bins=10):
    """LBP (uniform) -> résumé par max de bin et entropie de l'histogramme compressé."""
    lbp = local_binary_pattern(img_u8, P=P, R=R, method='uniform')
    bins = int(P + 2)  # nb "uniform patterns"
    hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins), density=True)
    if bins != n_bins:
        idxs = np.linspace(0, bins, n_bins + 1, dtype=int)
        hist_comp = np.array([hist[idxs[i]:idxs[i+1]].sum() for i in range(n_bins)])
    else:
        hist_comp = hist
    return {
        'lbp_maxbin': float(hist_comp.max()),
        'lbp_entropy': float(entropy(hist_comp + 1e-12))
    }

#measure edge density using Canny: proportion of “edge” pixels
def edge_density(img_u8, low_ratio=0.5, high_ratio=1.5):
    """Densité d'edges (Canny) = nb pixels edge / nb total."""
    med = np.median(img_u8)
    low = int(max(0, (1.0 - low_ratio) * med))
    high = int(min(255, (1.0 + high_ratio) * med))
    edges = cv2.Canny(img_u8, low, high)
    return {'edge_density': float((edges > 0).mean())}

#ummarize the global intensity distribution (after preprocessing)
def intensity_stats(img_u8):
    """Statistiques globales + entropie de l'histogramme."""
    x = img_u8.astype(np.float32) / 255.0
    h, _ = np.histogram(img_u8.ravel(), bins=256, range=(0, 255), density=True)
    return {
        'int_mean': float(x.mean()),
        'int_std': float(x.std()),
        'int_med': float(np.median(x)),
        'int_q75': float(np.quantile(x, 0.75)),
        'int_entropy': float(entropy(h + 1e-12))
    }

#apply all these functions to each image and return a DataFrame (1 row = 1 image)
def extract_features_batch(X_u8):
    rows = []
    for img in X_u8:
        f = {}
        f.update(glcm_features(img, n_levels=32))
        f.update(lbp_features(img, P=8, R=1.0, n_bins=10))
        f.update(edge_density(img))
        f.update(intensity_stats(img))
        rows.append(f)
    return pd.DataFrame(rows)

# ---------- 2) Features Extraction ----------
feat_train = extract_features_batch(X_train_u8)
feat_val   = extract_features_batch(X_val_u8)
feat_test  = extract_features_batch(X_test_u8)

feat_train['diagnosis'] = y_train_bin
feat_val['diagnosis']   = y_val_bin
feat_test['diagnosis']  = y_test_bin

print("Shapes features -> train/val/test:", feat_train.shape, feat_val.shape, feat_test.shape)
print(feat_train.head(2))

# ---------- 3) Pearson correlations with malignancy on the TRAIN set ----------
corr_train = feat_train.corr(numeric_only=True)['diagnosis'].sort_values(ascending=False)
print("\nCorrélations avec diagnosis (1=malin) - TRAIN (Top 20):\n", corr_train.head(20))
print("\nCorrélations négatives (Bottom 20):\n", corr_train.tail(20))

# ---------- 4) Automatic selection of the most correlated features ----------
# By default, we take the 4 highest in absolute value (at least 2)
corr_abs = feat_train.drop(columns=['diagnosis']).corrwith(feat_train['diagnosis']).abs().sort_values(ascending=False)
top_n = 4
chosen = list(corr_abs.index[:max(2, top_n)])  # garantit >=2
print("\nFeatures sélectionnées pour la LogReg:", chosen)

"""We keep only the 4 variables with the highest absolute correlation for the regression, but we might adjust this number later (more or fewer)"""

# ---------- 5) Preparation of matrices & standardization ----------
Xtr = feat_train[chosen].values
Xva = feat_val[chosen].values
Xte = feat_test[chosen].values

ytr = y_train_bin
yva = y_val_bin
yte = y_test_bin

scaler = StandardScaler()
Xtr_s = scaler.fit_transform(Xtr)   # fit on the training set only
Xva_s = scaler.transform(Xva)
Xte_s = scaler.transform(Xte)

# ---------- 6) Model: Logistic Regression ----------
clf = LogisticRegression(class_weight='balanced', max_iter=300, random_state=42)
clf.fit(Xtr_s, ytr)

def evaluate_split(Xs, y, name):
    proba = clf.predict_proba(Xs)[:, 1]
    pred  = (proba >= 0.5).astype(int)
    auc   = roc_auc_score(y, proba)
    acc   = accuracy_score(y, pred)
    f1    = f1_score(y, pred)
    cm    = confusion_matrix(y, pred)
    print(f"\n== {name} ==")
    print(f"AUC : {auc:.3f} | ACC : {acc:.3f} | F1 : {f1:.3f}")
    print("Matrice de confusion:\n", cm)
    print("Rapport de classification:\n", classification_report(y, pred, digits=3))
    return auc, acc, f1

_ = evaluate_split(Xtr_s, ytr, "TRAIN")
_ = evaluate_split(Xva_s, yva, "VAL")
_ = evaluate_split(Xte_s, yte, "TEST")

# =========================
# FEATURES IMPROVEMENT (BreastMNIST)
# =========================

# ---------- 1) Existing helper functions (quantif + basics) ----------
def quantize_ubyte(img_u8, n_levels=32):
    binsize = 256 // n_levels
    q = (img_u8 // binsize).astype(np.uint8)
    q[q >= n_levels] = n_levels - 1
    return q

def glcm_features_multi(img_u8, n_levels=32,
                        distances=(1,2,3,4,5),
                        angles=(0, np.pi/4, np.pi/2, 3*np.pi/4),
                        prefix="glcm"):
    q = quantize_ubyte(img_u8, n_levels=n_levels)
    glcm = graycomatrix(q, distances=distances, angles=angles, levels=n_levels,
                        symmetric=True, normed=True)
    feats = {}
    props = ['contrast','homogeneity','energy','correlation','ASM','dissimilarity']
    for prop in props:
        try:
            vals = graycoprops(glcm, prop)  # shape: (len(dist), len(angles))
            feats[f'{prefix}_{prop}_mean'] = float(vals.mean())
            feats[f'{prefix}_{prop}_std']  = float(vals.std())
            for i, d in enumerate(distances):
                feats[f'{prefix}_{prop}_d{d}_mean'] = float(vals[i,:].mean())
                feats[f'{prefix}_{prop}_d{d}_std']  = float(vals[i,:].std())
        except Exception:
            pass
    return feats

def lbp_features(img_u8, P=8, R=1.0, n_bins=10):
    lbp = local_binary_pattern(img_u8, P=P, R=R, method='uniform')
    bins = int(P + 2)
    hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins), density=True)
    if bins != n_bins:
        idxs = np.linspace(0, bins, n_bins + 1, dtype=int)
        hist_comp = np.array([hist[idxs[i]:idxs[i+1]].sum() for i in range(n_bins)])
    else:
        hist_comp = hist
    return {
        'lbp_maxbin': float(hist_comp.max()),
        'lbp_entropy': float(entropy(hist_comp + 1e-12))
    }

def edge_density(img_u8, low_ratio=0.5, high_ratio=1.5):
    med = np.median(img_u8)
    low = int(max(0, (1.0 - low_ratio) * med))
    high = int(min(255, (1.0 + high_ratio) * med))
    edges = cv2.Canny(img_u8, low, high)
    return {'edge_density': float((edges > 0).mean())}

def intensity_stats(img_u8):
    x = img_u8.astype(np.float32) / 255.0
    h, _ = np.histogram(img_u8.ravel(), bins=256, range=(0, 255), density=True)
    return {
        'int_mean': float(x.mean()),
        'int_std': float(x.std()),
        'int_med': float(np.median(x)),
        'int_q75': float(np.quantile(x, 0.75)),
        'int_entropy': float(entropy(h + 1e-12))
    }

# ---------- 2) New extractors (Gabor, Hu moments) ----------
def gabor_bank_features(img_u8,
                        thetas=(0, np.pi/4, np.pi/2, 3*np.pi/4),
                        freqs=(0.1, 0.2, 0.3),
                        prefix="gabor"):
    """
    Applique une banque de filtres de Gabor. On stocke moyennes/écarts-types
    des réponses en magnitude pour chaque (theta, freq) + global mean/std.
    """
    feats = {}
    responses = []
    for t in thetas:
        for f in freqs:
            real, imag = gabor(img_u8, frequency=f, theta=t)
            mag = np.hypot(real, imag).astype(np.float32)
            responses.append(mag)
            feats[f'{prefix}_mean_t{round(t,2)}_f{f}'] = float(mag.mean())
            feats[f'{prefix}_std_t{round(t,2)}_f{f}']  = float(mag.std())
    if responses:
        stack = np.stack(responses, axis=0)
        feats[f'{prefix}_global_mean'] = float(stack.mean())
        feats[f'{prefix}_global_std']  = float(stack.std())
    return feats

def hu_moments_features(img_u8, prefix="hu"):
    """
    Deux variantes:
    - Hu sur mask binaire (Otsu) => forme globale
    - Hu sur edges (Canny) => contour/irrégularité
    On renvoie les 7 moments de Hu (log-transformés).
    """
    # 1) Otsu mask
    blur = cv2.GaussianBlur(img_u8, (5,5), 0)
    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
    m = cv2.moments(mask)
    hu = cv2.HuMoments(m).ravel()
    hu = -np.sign(hu) * np.log10(np.abs(hu) + 1e-12)  # log transform standard
    feats = {f'{prefix}_bin_{i+1}': float(hu[i]) for i in range(7)}

    # 2) Edges mask
    med = np.median(img_u8)
    low = int(max(0, 0.5 * med))
    high = int(min(255, 1.5 * med))
    edges = cv2.Canny(img_u8, low, high)
    m2 = cv2.moments(edges)
    hu2 = cv2.HuMoments(m2).ravel()
    hu2 = -np.sign(hu2) * np.log10(np.abs(hu2) + 1e-12)
    feats.update({f'{prefix}_edge_{i+1}': float(hu2[i]) for i in range(7)})

    return feats

# ---------- 3) Feature combinations (ratios/products) ----------
def combine_features_scalar(feat_row):
    """
    Crée des interactions simples entre indicateurs clés si disponibles.
    On protège par try/except au cas où certaines clés n'existent pas.
    """
    out = {}
    def g(key, default=np.nan): return feat_row.get(key, default)

    # Examples:
    # texture complexity (contrast/homogeneity)
    c_mean = g('glcm_contrast_mean')
    h_mean = g('glcm_homogeneity_mean')
    if c_mean is not np.nan and h_mean not in (0, np.nan):
        out['combo_contrast_over_homog'] = float(c_mean / (h_mean + 1e-6))

    # texture variability × edges
    c_std = g('glcm_contrast_std')
    ed    = g('edge_density')
    if not np.isnan(c_std) and not np.isnan(ed):
        out['combo_edge_x_contrast_std'] = float(ed * c_std)

    # dissimilarity mean/std en ratios
    d_mean = g('glcm_dissimilarity_mean')
    d_std  = g('glcm_dissimilarity_std')
    if d_mean not in (0, np.nan) and not np.isnan(d_std):
        out['combo_dissim_std_over_mean'] = float(d_std / (d_mean + 1e-6))

    # global entropy × edge density
    int_ent = g('int_entropy')
    if not np.isnan(int_ent) and not np.isnan(ed):
        out['combo_edge_x_int_entropy'] = float(ed * int_ent)

    # gabor global vs int_std
    gab_g_std = g('gabor_global_std')
    int_std   = g('int_std')
    if not np.isnan(gab_g_std) and not np.isnan(int_std):
        out['combo_gaborstd_over_intstd'] = float(gab_g_std / (int_std + 1e-6))

    return out

# ---------- 4) Extended batch extraction ----------
def extract_features_batch_extended(X_u8):
    rows = []
    for img in X_u8:
        f = {}
        # Multi-scale texture (GLCM distances 1..5)
        f.update(glcm_features_multi(img, n_levels=32,
                                     distances=(1,2,3,4,5),
                                     angles=(0, np.pi/4, np.pi/2, 3*np.pi/4),
                                     prefix="glcm"))

        # Local micro-texture (LBP)
        f.update(lbp_features(img, P=8, R=1.0, n_bins=10))

        # Edges / contours
        f.update(edge_density(img))

        # Intensity statistics
        f.update(intensity_stats(img))

        # Gabor (Frequencies and orientations)
        f.update(gabor_bank_features(img, thetas=(0, np.pi/4, np.pi/2, 3*np.pi/4),
                                     freqs=(0.1, 0.2, 0.3), prefix="gabor"))

        # Shape moments (binary + edges)
        f.update(hu_moments_features(img, prefix="hu"))

        # Combinations (at the end, since they depend on previous keys)
        f.update(combine_features_scalar(f))

        rows.append(f)
    return pd.DataFrame(rows)

# ---------- 5) Feature extraction on train/val/test ----------
feat_train_A = extract_features_batch_extended(X_train_u8)
feat_val_A   = extract_features_batch_extended(X_val_u8)
feat_test_A  = extract_features_batch_extended(X_test_u8)

feat_train_A['diagnosis'] = y_train_bin
feat_val_A['diagnosis']   = y_val_bin
feat_test_A['diagnosis']  = y_test_bin

print("Shapes (A) -> train/val/test:", feat_train_A.shape, feat_val_A.shape, feat_test_A.shape)
print("Preview (train):\n", feat_train_A.head(2))

# ---------- 6) Feature selection (correlation + ANOVA + MI + RFE) ----------
# 6.1 Absolute correlation
corr_abs_A = feat_train_A.drop(columns=['diagnosis']).corrwith(feat_train_A['diagnosis']).abs().sort_values(ascending=False)
top_corr = list(corr_abs_A.index[:20])  # top 20 corrélées (ajuste si besoin)

# 6.2 Univariate selections (ANOVA & Mutual Information)
Xtr_A = feat_train_A.drop(columns=['diagnosis']).values
ytr_A = y_train_bin
feature_names_A = feat_train_A.drop(columns=['diagnosis']).columns.tolist()

# ANOVA F
skb_f = SelectKBest(score_func=f_classif, k=min(20, Xtr_A.shape[1]))
skb_f.fit(Xtr_A, ytr_A)
top_anova_idx = np.argsort(skb_f.scores_)[::-1][:20]
top_anova = [feature_names_A[i] for i in top_anova_idx]

# Mutual information
skb_mi = SelectKBest(score_func=mutual_info_classif, k=min(20, Xtr_A.shape[1]))
skb_mi.fit(Xtr_A, ytr_A)
top_mi_idx = np.argsort(skb_mi.scores_)[::-1][:20]
top_mi = [feature_names_A[i] for i in top_mi_idx]

# 6.3 RFE (LogReg) — simple, fast
est = LogisticRegression(max_iter=500, class_weight='balanced', solver='liblinear')
rfe = RFE(estimator=est, n_features_to_select=min(15, Xtr_A.shape[1]), step=1)
rfe.fit(Xtr_A, ytr_A)
top_rfe = [name for name, keep in zip(feature_names_A, rfe.support_) if keep]

# 6.4 Union (consensus) of selections
selected_set = set(top_corr[:15]) | set(top_anova[:15]) | set(top_mi[:15]) | set(top_rfe)
selected_features_final = sorted(selected_set)  # liste finale ordonnée
print("\n[PLAN A] Selected features (consensus):", len(selected_features_final))
print(selected_features_final)

# ---------- 7) Final matrices ready for Logistic Regression ----------
Xtr_sel_A = feat_train_A[selected_features_final].values
Xva_sel_A = feat_val_A[selected_features_final].values
Xte_sel_A = feat_test_A[selected_features_final].values

# --- 0) Selected feature matrices ---
Xtr = feat_train_A[selected_features_final].values
Xva = feat_val_A[selected_features_final].values
Xte = feat_test_A[selected_features_final].values

ytr = y_train_bin
yva = y_val_bin
yte = y_test_bin

# --- 1) Standardization (fit on TRAIN only) ---
scaler = StandardScaler()
Xtr_s = scaler.fit_transform(Xtr)
Xva_s = scaler.transform(Xva)
Xte_s = scaler.transform(Xte)

# --- 2) Model: Logistic Regression (majority/balanced class) ---
clf = LogisticRegression(class_weight='balanced', max_iter=500, random_state=42, solver='liblinear')
clf.fit(Xtr_s, ytr)

def evaluate_split(Xs, y, name, threshold=0.5, verbose=True):
    proba = clf.predict_proba(Xs)[:, 1]
    pred  = (proba >= threshold).astype(int)
    auc   = roc_auc_score(y, proba)
    acc   = accuracy_score(y, pred)
    f1    = f1_score(y, pred)
    cm    = confusion_matrix(y, pred)
    if verbose:
        print(f"\n== {name} @th={threshold:.3f} ==")
        print(f"AUC : {auc:.3f} | ACC : {acc:.3f} | F1 : {f1:.3f}")
        print("Matrice de confusion:\n", cm)
        print("Rapport de classification:\n", classification_report(y, pred, digits=3))
    return auc, acc, f1, proba, pred

# --- 3) Evaluation at threshold 0.5 ---
_ = evaluate_split(Xtr_s, ytr, "TRAIN", threshold=0.5, verbose=True)
_ = evaluate_split(Xva_s, yva, "VAL",   threshold=0.5, verbose=True)
_ = evaluate_split(Xte_s, yte, "TEST",  threshold=0.5, verbose=True)

# --- 4) Selection of a threshold on the validation set to maximize F1 ---
prec, rec, thr = precision_recall_curve(yva, clf.predict_proba(Xva_s)[:, 1])
f1s = 2 * prec * rec / (prec + rec + 1e-12)
best_idx = np.nanargmax(f1s)
best_th = thr[best_idx] if best_idx < len(thr) else 0.5
print(f"\nSeuil optimisé sur VAL pour max F1 : th* = {best_th:.3f}")

# Re-evaluation with optimized threshold
_ = evaluate_split(Xtr_s, ytr, "TRAIN (th*)", threshold=best_th, verbose=True)
_ = evaluate_split(Xva_s, yva, "VAL   (th*)", threshold=best_th, verbose=True)
_ = evaluate_split(Xte_s, yte, "TEST  (th*)", threshold=best_th, verbose=True)

# --- 5) Interpretation of coefficients ---
coef_df = pd.DataFrame({
    'feature': selected_features_final,
    'coef': clf.coef_.ravel()
}).sort_values('coef', ascending=False)
print("\nCoefficients LogReg (positif => ↑ prob. malignité):\n", coef_df.reset_index(drop=True))

"""## Part 3 : Modeling

1. Environment setup: reproducibility block (to obtain the same results on every run):
We fix the random seeds for random (Python’s random functions), numpy, and torch (for PyTorch), and disable certain non-deterministic optimizations in PyTorch.
"""

# --------- SEEDS & DETERMINISM ---------
SEED = 42
def set_all_seeds(seed=SEED):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
set_all_seeds()

"""2. LinearSVM training"""

# ========= CALIBRATED SVM (on handcrafted features) =========

# We start again from the standardized matrices: Xtr_s, Xva_s, Xte_s and ytr, yva, yte
svm_base = LinearSVC(C=1.0, class_weight='balanced', random_state=SEED)
svm = CalibratedClassifierCV(svm_base, method='sigmoid', cv=5)
svm.fit(Xtr_s, ytr)

def eval_model_clf(model, X, y, name, th=0.5):
    proba = model.predict_proba(X)[:,1]
    pred  = (proba >= th).astype(int)
    auc   = roc_auc_score(y, proba)
    acc   = accuracy_score(y, pred)
    f1    = f1_score(y, pred)
    cm    = confusion_matrix(y, pred)
    print(f"\n=== {name} @th={th:.3f} ===")
    print(f"AUC: {auc:.3f} | ACC: {acc:.3f} | F1: {f1:.3f}")
    print("Confusion matrix:\n", cm)
    print("Classification report:\n", classification_report(y, pred, digits=3))
    return {"auc":auc, "acc":acc, "f1":f1, "cm":cm}

_ = eval_model_clf(svm, Xtr_s, ytr, "Linear SVM [TRAIN]")
_ = eval_model_clf(svm, Xva_s, yva, "Linear SVM [VAL]")
_ = eval_model_clf(svm, Xte_s, yte, "Linear SVM [TEST]")

# ==== SVM CALIBRÉE + seuil optimisé F1 sur VALIDATION ====

def th_maxF1(y_true, proba):
    prec, rec, thr = precision_recall_curve(y_true, proba)
    f1s = 2 * prec * rec / (prec + rec + 1e-12)
    i = int(np.nanargmax(f1s))
    return float(thr[i]) if i < len(thr) else 0.5

# 1) SVM linéaire + calibration sigmoïde (CV stratifiée conseillée)
cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
svm_base = LinearSVC(C=1.0, class_weight='balanced', random_state=SEED)
svm = CalibratedClassifierCV(svm_base, method='sigmoid', cv=cv5)
svm.fit(Xtr_s, ytr)

# 2) Seuil optimisé F1 sur la VAL
p_va = svm.predict_proba(Xva_s)[:, 1]
th_f1_val = th_maxF1(yva, p_va)
print(f"[SVM cal.] Seuil optimisé F1 sur VAL : th* = {th_f1_val:.4f}")

# 3) Évaluations @0.5 (baseline) pour comparaison
_ = eval_model_clf(svm, Xte_s, yte, "Linear SVM (cal.) [TEST]  @0.5", th=0.5)

# 4) Évaluations avec le seuil gelé F1@VAL (à reporter pour TEST)
_ = eval_model_clf(svm, Xte_s, yte, "Linear SVM (cal.) [TEST]  @F1(VAL)", th=th_f1_val)

"""3. LR"""

lr = LogisticRegression(
    penalty="l2",
    C=1.0,
    class_weight="balanced",
    solver="liblinear",
    max_iter=500,
    random_state=SEED,
)

lr.fit(Xtr_s, ytr)

# Evaluations
_ = eval_model_clf(lr, Xte_s, yte, "Logistic Regression [TEST]",  th=0.5)

def th_maxF1(y_true, proba):
    """Retourne (seuil_F1_max, F1_au_seuil) calculés sur y_true/proba."""
    prec, rec, thr = precision_recall_curve(y_true, proba)
    f1s = 2 * prec * rec / (prec + rec + 1e-12)
    i = int(np.nanargmax(f1s))
    best_th = float(thr[i]) if i < len(thr) else 0.5  # garde 0.5 si pas de seuil dispo
    best_f1 = float(np.nanmax(f1s))
    return best_th, best_f1

# 1) Entraînement LR (comme dans ta partie 3)
lr = LogisticRegression(
    penalty="l2",
    C=1.0,
    class_weight="balanced",
    solver="liblinear",
    max_iter=500,
    random_state=SEED,
)
lr.fit(Xtr_s, ytr)

# 2) Seuil optimisé F1 sur la VAL
p_va = lr.predict_proba(Xva_s)[:, 1]
th_f1_val, f1_val_at_th = th_maxF1(yva, p_va)
print(f"[LR] Seuil optimisé F1 sur VAL : th* = {th_f1_val:.4f} (F1_VAL={f1_val_at_th:.3f})")

# 3) Évaluations @0.5 (baseline) pour comparaison
_ = eval_model_clf(lr, Xte_s, yte, "Logistic Regression [TEST]  @0.5", th=0.5)

# 4) Évaluations avec le seuil gelé F1@VAL (ce qu'on rapporte pour TEST)
_ = eval_model_clf(lr, Xte_s, yte, "Logistic Regression [TEST]  @F1(VAL)", th=th_f1_val)

"""3. CNN -- DenseNet121 Training

"""

# ========= DenseNet121 (transfer) =========
warnings.filterwarnings("ignore")

# ---------- Quick config ----------
RESIZE = 128
BATCH_SIZE = 48
FREEZE_EPOCHS = 6
FINETUNE_EPOCHS = 18
LR_HEAD = 3e-4
LR_FEATS = 1e-4
WEIGHT_DECAY = 1e-4
MIXUP_ALPHA = 0.2
MIXUP_PROB = 0.8
LABEL_SMOOTH_EPS = 0.05
GRAD_CLIP_NORM = 1.0

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------- 0) ALIGNMENT images/labels ----------
try:
    y_train_cnn = y_train_all
except NameError:
    y_train_cnn = y_train

if len(X_train_norm) != len(y_train_cnn):
    if len(X_train_norm) % len(y_train_cnn) == 0:
        reps = len(X_train_norm) // len(y_train_cnn)
        y_train_cnn = np.tile(y_train_cnn, reps)
        print(f"[Align] y_train répété {reps}×")
    else:
        n = min(len(X_train_norm), len(y_train_cnn))
        X_train_norm = X_train_norm[:n]; y_train_cnn = y_train_cnn[:n]
        print(f"[Align] tronqué train à {n}")

print("CNN train shapes:", X_train_norm.shape, y_train_cnn.shape)

# ---------- 1) Torch dataset (3 channels for ImageNet) ----------
class BreastNPZDataset(Dataset):
    def __init__(self, X, y, resize_to=128, repeat_3ch=True, aug=True):
        X = X.astype(np.float32); y = y.astype(np.uint8)
        N = min(len(X), len(y))
        self.X, self.y = X[:N], y[:N]
        self.resize_to = resize_to
        self.repeat_3ch = repeat_3ch
        self.aug = aug
        self.N = N
    def __len__(self): return self.N

    def _augment(self, img):  # (1,H,W)
        if torch.rand(1).item() < 0.5:
            img = torch.flip(img, dims=[2])
        angle = (torch.rand(1).item()*20.0 - 10.0) * math.pi/180.0
        theta = torch.tensor([[math.cos(angle), -math.sin(angle), 0.0],
                              [math.sin(angle),  math.cos(angle), 0.0]], dtype=torch.float32).unsqueeze(0)
        grid = F.affine_grid(theta, size=(1,1,img.shape[1],img.shape[2]), align_corners=False)
        img  = F.grid_sample(img.unsqueeze(0), grid, mode='bilinear', padding_mode='zeros',
                             align_corners=False).squeeze(0)
        if torch.rand(1).item() < 0.5:
            img = img * (0.9 + 0.2*torch.rand(1).item())
        if torch.rand(1).item() < 0.5:
            img = img + (torch.randn_like(img)*0.05).clamp(-0.1,0.1)
        return img

    def __getitem__(self, idx):
        x = torch.from_numpy(self.X[idx]).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)
        if x.shape[-2:] != (self.resize_to, self.resize_to):
            x = F.interpolate(x, size=(self.resize_to, self.resize_to),
                              mode='bilinear', align_corners=False)
        x = x.squeeze(0)  # (1,H,W)
        if self.aug: x = self._augment(x)
        if self.repeat_3ch: x = x.repeat(3,1,1)                      # (3,H,W)
        y = int(self.y[idx])
        return x, y

def _worker_init_fn(worker_id):
    np.random.seed(SEED + worker_id)

def make_loaders(resize_to=RESIZE, batch_size=BATCH_SIZE, aug_train=True):
    g = torch.Generator(); g.manual_seed(SEED)
    ds_tr = BreastNPZDataset(X_train_norm, y_train_cnn, resize_to=resize_to, aug=aug_train)
    ds_va = BreastNPZDataset(X_val_norm,   y_val,      resize_to=resize_to, aug=False)
    ds_te = BreastNPZDataset(X_test_norm,  y_test,     resize_to=resize_to, aug=False)
    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True,
                       worker_init_fn=_worker_init_fn, generator=g)
    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True,
                       worker_init_fn=_worker_init_fn, generator=g)
    dl_te = DataLoader(ds_te, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True,
                       worker_init_fn=_worker_init_fn, generator=g)
    return dl_tr, dl_va, dl_te

# ---------- 2) Pretrained DenseNet121 ----------
from torchvision import models
def make_densenet121(num_classes=1):
    m = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)
    in_feats = m.classifier.in_features
    m.classifier = nn.Linear(in_feats, num_classes)
    return m

# ---------- 3) Mixup + Label smoothing ----------
def mixup_batch(x, y, alpha=MIXUP_ALPHA):
    if alpha <= 0 or len(x) < 2: return x, y, 1.0
    lam = np.random.beta(alpha, alpha)
    indices = torch.randperm(len(x), device=x.device)
    x_mix = lam * x + (1 - lam) * x[indices]
    y = y.float().unsqueeze(1)
    y_mix = lam * y + (1 - lam) * y[indices]
    return x_mix, y_mix.squeeze(1), lam

def smooth_labels(y, eps=LABEL_SMOOTH_EPS):
    return y*(1 - eps) + 0.5*eps

# ---------- 4) Training (freeze → unfreeze) ----------
def set_trainable(module, flag: bool):
    for p in module.parameters(): p.requires_grad = flag

def bce_logits_loss(logits, targets, pos_weight=None):
    return F.binary_cross_entropy_with_logits(logits, targets, pos_weight=pos_weight)

def train_epoch(model, loader, opt, pos_weight=None, apply_mixup=True):
    model.train()
    losses = []
    for xb, yb in loader:
        xb = xb.to(DEVICE); yb = yb.to(DEVICE).float()
        if apply_mixup and torch.rand(1).item() < MIXUP_PROB:
            xb, yb, _ = mixup_batch(xb, yb, MIXUP_ALPHA)
        yb_s = smooth_labels(yb, LABEL_SMOOTH_EPS)
        logits = model(xb).squeeze(1)
        loss = bce_logits_loss(logits, yb_s, pos_weight=pos_weight)
        opt.zero_grad(); loss.backward()
        if GRAD_CLIP_NORM is not None:
            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
        opt.step()
        losses.append(loss.item())
    return float(np.mean(losses))

@torch.no_grad()
def evaluate_probs(model, loader):
    model.eval()
    probs, ys = [], []
    for xb, yb in loader:
        logits = model(xb.to(DEVICE)).squeeze(1)
        p = torch.sigmoid(logits).cpu().numpy()
        probs.append(p); ys.append(yb.numpy())
    return np.concatenate(probs), np.concatenate(ys)

def optimize_threshold(y_val, p_val):
    prec, rec, thr = precision_recall_curve(y_val, p_val)
    f1s = 2 * prec * rec / (prec + rec + 1e-12)
    bi = np.nanargmax(f1s)
    return thr[bi] if bi < len(thr) else 0.5

def metrics_block(y, p, name, th):
    pred = (p >= th).astype(int)
    auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else float("nan")
    acc = accuracy_score(y, pred); f1 = f1_score(y, pred)
    cm  = confusion_matrix(y, pred)
    print(f"\n== {name} (threshold={th:.3f}) ==")
    print(f"AUROC={auc:.3f} | ACC={acc:.3f} | F1={f1:.3f}")
    print("Confusion matrix:\n", cm)
    print("Classification report:\n", classification_report(y, pred, digits=3))
    return {"auc":auc, "acc":acc, "f1":f1, "cm":cm}

# ---------- 5) Start training ----------
dl_tr, dl_va, dl_te = make_loaders(RESIZE, BATCH_SIZE, aug_train=True)

# pos_weight — to maximize recall of class 1 (majority here), we choose None
n_pos = int((y_train_cnn==1).sum()); n_neg = int((y_train_cnn==0).sum())
pos_weight = None
print(f"[Class balance] pos={n_pos}, neg={n_neg} (pos_weight=None)")

model = make_densenet121().to(DEVICE)

# Phase 1 — FREEZE
set_trainable(model.features, False)
set_trainable(model.classifier, True)
opt = torch.optim.AdamW([{"params": model.classifier.parameters(), "lr": LR_HEAD, "weight_decay": WEIGHT_DECAY}], lr=LR_HEAD)

best_val_auc, best_state = -1.0, None
wait, patience = 4, 4
for ep in range(1, FREEZE_EPOCHS+1):
    tl = train_epoch(model, dl_tr, opt, pos_weight=pos_weight, apply_mixup=True)
    p_va, y_va = evaluate_probs(model, dl_va)
    auc = roc_auc_score(y_va, p_va) if len(np.unique(y_va))>1 else float('nan')
    print(f"[FREEZE {ep:02d}/{FREEZE_EPOCHS}] loss={tl:.4f} | val_AUROC={auc:.4f}")
    if auc > best_val_auc:
        best_val_auc, best_state = auc, copy.deepcopy(model.state_dict()); wait = 0
    else:
        wait += 1
        if wait >= patience:
            print("  ↳ early stop (warmup)"); break
if best_state is not None:
    model.load_state_dict(best_state)

# Phase 2 — FINETUNE
set_trainable(model.features, True)
set_trainable(model.classifier, True)
opt = torch.optim.AdamW([
    {"params": model.features.parameters(),   "lr": LR_FEATS, "weight_decay": WEIGHT_DECAY},
    {"params": model.classifier.parameters(), "lr": LR_HEAD,  "weight_decay": WEIGHT_DECAY},
])

best_val_auc, best_state = -1.0, None
patience = 7; wait = 0
for ep in range(1, FINETUNE_EPOCHS+1):
    tl = train_epoch(model, dl_tr, opt, pos_weight=pos_weight, apply_mixup=True)
    p_va, y_va = evaluate_probs(model, dl_va)
    auc = roc_auc_score(y_va, p_va) if len(np.unique(y_va))>1 else float('nan')
    print(f"[FINETUNE {ep:02d}/{FINETUNE_EPOCHS}] loss={tl:.4f} | val_AUROC={auc:.4f}")
    if auc > best_val_auc:
        best_val_auc, best_state = auc, copy.deepcopy(model.state_dict()); wait = 0
    else:
        wait += 1
        if wait >= patience:
            print("  ↳ early stopping (finetune)"); break
if best_state is not None:
    model.load_state_dict(best_state)

# ---------- 6) EVALUATION — Seuils calibrés sur la validation ----------

@torch.no_grad()
def evaluate_probs(model, loader):
    model.eval()
    probs, ys = [], []
    for xb, yb in loader:
        logits = model(xb.to(DEVICE)).squeeze(1)
        p = torch.sigmoid(logits).cpu().numpy()
        probs.append(p); ys.append(yb.numpy())
    return np.concatenate(probs), np.concatenate(ys)

def metrics_block(y, p, name, th):
    pred = (p >= th).astype(int)
    auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else float("nan")
    acc = accuracy_score(y, pred); f1 = f1_score(y, pred)
    cm  = confusion_matrix(y, pred)
    tn, fp, fn, tp = cm.ravel()
    recall = tp / (tp + fn + 1e-12)
    specificity = tn / (tn + fp + 1e-12)
    precision = tp / (tp + fp + 1e-12)
    print(f"\n== {name} (th={th:.4f}) ==")
    print(f"AUROC={auc:.3f} | ACC={acc:.3f} | F1={f1:.3f} | "
          f"Recall={recall:.3f} | Spec={specificity:.3f} | Prec={precision:.3f}")
    print("Confusion matrix:\n", cm)
    print("Classification report:\n", classification_report(y, pred, digits=3))
    return {"auc":auc, "acc":acc, "f1":f1, "recall":recall, "spec":specificity, "prec":precision, "cm":cm}

# --- Seuils sur la validation ---
from sklearn.metrics import roc_curve, precision_recall_curve

def threshold_f1(y, p):
    prec, rec, thr = precision_recall_curve(y, p)
    f1s = 2 * prec * rec / (prec + rec + 1e-12)
    i = np.nanargmax(f1s)
    return thr[i] if i < len(thr) else 0.5

def threshold_youdenJ(y, p):
    fpr, tpr, thr = roc_curve(y, p)
    j = tpr - fpr
    return thr[np.argmax(j)]

def threshold_recall_at_max_fpr(y, p, max_fpr=0.10):
    """Parmi les points ROC avec FPR <= max_fpr, prendre celui qui maximise le rappel (TPR)."""
    fpr, tpr, thr = roc_curve(y, p)
    ok = np.where(fpr <= max_fpr)[0]
    if len(ok) == 0:
        # fallback: meilleur Youden J
        return threshold_youdenJ(y, p)
    best = ok[np.argmax(tpr[ok])]
    return thr[best]

def threshold_min_fpr_at_recall(y, p, recall_target=0.95):
    """Parmi les points ROC avec TPR >= recall_target, prendre celui qui minimise FPR."""
    fpr, tpr, thr = roc_curve(y, p)
    ok = np.where(tpr >= recall_target)[0]
    if len(ok) == 0:
        # fallback: plus haut TPR possible
        best = np.argmax(tpr)
        return thr[best]
    best = ok[np.argmin(fpr[ok])]
    return thr[best]

# --- Probabilités ---
p_tr, y_tr = evaluate_probs(model, dl_tr)
p_va, y_va = evaluate_probs(model, dl_va)
p_te, y_te = evaluate_probs(model, dl_te)

# --- Calibrage du seuil sur la validation ---
MAX_FPR = 0.10        # contrainte de faux positifs (modifiable)
RECALL_TARGET = 0.95  # cible de rappel (option B)

th_f1   = threshold_f1(y_va, p_va)
th_j    = threshold_youdenJ(y_va, p_va)
th_fpr  = threshold_recall_at_max_fpr(y_va, p_va, max_fpr=MAX_FPR)      # **choix par défaut**
th_rec  = threshold_min_fpr_at_recall(y_va, p_va, recall_target=RECALL_TARGET)

print("\n[Seuils calibrés sur VALIDATION]")
print(f" - F1 max          : {th_f1:.4f}")
print(f" - Youden J max    : {th_j:.4f}")
print(f" - Rappel max @FPR<={MAX_FPR:.0%} : {th_fpr:.4f}   <-- recommandé")
print(f" - Min FPR @Recall>={RECALL_TARGET:.0%} : {th_rec:.4f}")

# --- Choix du seuil final ---
TH = th_fpr   # <== choisis la stratégie que tu veux (th_f1 / th_j / th_fpr / th_rec)

# --- Évaluations ---
_ = metrics_block(y_tr, p_tr, "TRAIN (DenseNet121)", TH)
_ = metrics_block(y_va, p_va, "VAL   (DenseNet121)", TH)
_ = metrics_block(y_te, p_te, "TEST  (DenseNet121)", TH)

print("\n===== RÉSUMÉ TEST (DenseNet121, resize=%d) =====" % RESIZE)
print(f"[Seuil choisi {TH:.4f}]")

TH = th_f1

print(f"\n[Évaluation au seuil F1_max = {TH:.4f}]")
_ = metrics_block(y_tr, p_tr, "TRAIN (DenseNet121)", TH)
_ = metrics_block(y_va, p_va, "VAL   (DenseNet121)", TH)
_ = metrics_block(y_te, p_te, "TEST  (DenseNet121)", TH)

# FPR sur TEST pour bien voir le trade-off
from sklearn.metrics import confusion_matrix
pred_te = (p_te >= TH).astype(int)
tn, fp, fn, tp = confusion_matrix(y_te, pred_te).ravel()
fpr = fp / (fp + tn + 1e-12)
tpr = tp / (tp + fn + 1e-12)
print(f"[TEST] Recall={tpr:.3f} | FPR={fpr:.3f}")

# Evaluation rapide du CNN @0.5 (TEST)
summary_rows.append(evaluate_point(y_te, p_te, 0.5, "CNN DenseNet121 [TEST] @0.5"))

"""Part 4 : Evaluation"""

# ---------- 4.0 — Helpers communs ----------
def th_maxF1(y_true, proba):
    prec, rec, thr = precision_recall_curve(y_true, proba)
    f1s = 2 * prec * rec / (prec + rec + 1e-12)
    i = int(np.nanargmax(f1s))
    return float(thr[i]) if i < len(thr) else 0.5

def evaluate_point(y_true, proba, th, name):
    """Imprime classification report + AUC, retourne un dict de métriques clés."""
    pred = (proba >= th).astype(int)
    auc = roc_auc_score(y_true, proba) if len(np.unique(y_true))>1 else float("nan")
    acc = accuracy_score(y_true, pred)
    f1  = f1_score(y_true, pred)
    print(f"\n== {name} (th={th:.4f}) ==")
    print(f"AUC={auc:.3f} | ACC={acc:.3f} | F1={f1:.3f}")
    print(classification_report(y_true, pred, digits=3))
    return {"Model": name, "Threshold": th, "AUC": auc, "ACC": acc, "F1": f1}

summary_rows = []

# ---------- 4.1 — LOGISTIC REGRESSION ----------
# probabilités TEST
p_te_lr = lr.predict_proba(Xte_s)[:, 1]
# seuil optimisé F1 sur VAL
p_va_lr = lr.predict_proba(Xva_s)[:, 1]
th_lr_f1 = th_maxF1(yva, p_va_lr)

# rapports & AUC
summary_rows.append(evaluate_point(yte, p_te_lr, 0.5,         "LR [TEST] @0.5"))
summary_rows.append(evaluate_point(yte, p_te_lr, th_lr_f1,     "LR [TEST] @F1(VAL)"))

# ---------- 4.2 — SVM LINÉAIRE CALIBRÉE ----------
# probabilités TEST
p_te_svm = svm.predict_proba(Xte_s)[:, 1]
# seuil optimisé F1 sur VAL
p_va_svm = svm.predict_proba(Xva_s)[:, 1]
th_svm_f1 = th_maxF1(yva, p_va_svm)

# rapports & AUC
summary_rows.append(evaluate_point(yte, p_te_svm, 0.5,         "SVM cal. [TEST] @0.5"))
summary_rows.append(evaluate_point(yte, p_te_svm, th_svm_f1,    "SVM cal. [TEST] @F1(VAL)"))

# ---------- 4.3 — CNN (DenseNet121) ----------

import copy
from sklearn.metrics import roc_auc_score

def train_cnn_for_size(resize:int):
    """Train DenseNet121 with the same pipeline at a given resize, return trained model and (p_tr, y_tr, p_va, y_va, p_te, y_te)."""
    # 1) loaders
    dl_tr, dl_va, dl_te = make_loaders(resize_to=resize, batch_size=BATCH_SIZE, aug_train=True)

    # 2) model
    model = make_densenet121().to(DEVICE)

    # 3) Phase 1 — freeze head
    set_trainable(model.features, False)
    set_trainable(model.classifier, True)
    opt = torch.optim.AdamW(
        [{"params": model.classifier.parameters(), "lr": LR_HEAD, "weight_decay": WEIGHT_DECAY}],
        lr=LR_HEAD
    )

    best_val_auc, best_state = -1.0, None
    wait, patience = 4, 4
    for ep in range(1, FREEZE_EPOCHS+1):
        _ = train_epoch(model, dl_tr, opt, pos_weight=None, apply_mixup=True)
        p_va, y_va = evaluate_probs(model, dl_va)
        auc = roc_auc_score(y_va, p_va) if len(np.unique(y_va))>1 else float("nan")
        print(f"[{resize}px][FREEZE {ep:02d}/{FREEZE_EPOCHS}] val_AUROC={auc:.4f}")
        if auc > best_val_auc:
            best_val_auc, best_state = auc, copy.deepcopy(model.state_dict()); wait = 0
        else:
            wait += 1
            if wait >= patience:
                print(f"  ↳ early stop (warmup) @ {resize}px"); break
    if best_state is not None:
        model.load_state_dict(best_state)

    # 4) Phase 2 — finetune all
    set_trainable(model.features, True)
    set_trainable(model.classifier, True)
    opt = torch.optim.AdamW([
        {"params": model.features.parameters(),   "lr": LR_FEATS, "weight_decay": WEIGHT_DECAY},
        {"params": model.classifier.parameters(), "lr": LR_HEAD,  "weight_decay": WEIGHT_DECAY},
    ])

    best_val_auc, best_state = -1.0, None
    patience, wait = 7, 0
    for ep in range(1, FINETUNE_EPOCHS+1):
        _ = train_epoch(model, dl_tr, opt, pos_weight=None, apply_mixup=True)
        p_va, y_va = evaluate_probs(model, dl_va)
        auc = roc_auc_score(y_va, p_va) if len(np.unique(y_va))>1 else float("nan")
        print(f"[{resize}px][FINETUNE {ep:02d}/{FINETUNE_EPOCHS}] val_AUROC={auc:.4f}")
        if auc > best_val_auc:
            best_val_auc, best_state = auc, copy.deepcopy(model.state_dict()); wait = 0
        else:
            wait += 1
            if wait >= patience:
                print(f"  ↳ early stopping (finetune) @ {resize}px"); break
    if best_state is not None:
        model.load_state_dict(best_state)

    # 5) probs
    p_tr, y_tr = evaluate_probs(model, dl_tr)
    p_va, y_va = evaluate_probs(model, dl_va)
    p_te, y_te = evaluate_probs(model, dl_te)
    return model, (p_tr, y_tr, p_va, y_va, p_te, y_te)

def evaluate_cnn_size(resize:int, tag:str):
    """Train + evaluate at a specific resize, print reports, append to summary_rows at @0.5 and @F1(VAL)."""
    model, (p_tr, y_tr, p_va, y_va, p_te, y_te) = train_cnn_for_size(resize)
    # threshold from validation (F1-max)
    th_f1 = th_maxF1(y_va, p_va)
    print(f"\n[{tag}] Selected threshold on VALIDATION (F1_max): {th_f1:.4f}")

    # reports on TEST
    summary_rows.append(evaluate_point(y_te, p_te, 0.5,          f"{tag} [TEST] @0.5"))
    summary_rows.append(evaluate_point(y_te, p_te, th_f1,        f"{tag} [TEST] @F1(VAL)"))

    # (optional) also show TRAIN/VAL reports at the chosen threshold, for completeness
    _ = evaluate_point(y_tr, p_tr, th_f1, f"{tag} [TRAIN] @F1(VAL)")
    _ = evaluate_point(y_va, p_va, th_f1, f"{tag} [VAL]   @F1(VAL)")

# ---- Run the two sizes with the exact same pipeline ----
evaluate_cnn_size(128, "CNN DenseNet121 (128×128)")
evaluate_cnn_size(224, "CNN DenseNet121 (224×224)")

# ---- Summary table across all models (your classic models + CNNs) ----
# Convert to a nice dataframe if you want:
try:
    import pandas as pd
    df_summary = pd.DataFrame(summary_rows)
    display(df_summary)
except Exception:
    pass

